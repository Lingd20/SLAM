{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898410a4-7512-43c0-bcd6-ee6daa8652d9",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "cd transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ca2b9d-67b9-4841-904e-2497b2da4cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f412b60e-76dc-40d7-af96-61e15c266a6f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "!git checkout tags/v4.35.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54455652",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip show transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdba35ec-495f-45ca-afb3-b4cc1ef028a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2052fec-e6c5-4bd7-9fb7-9e6ff91dd21e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29cde01-5f51-45d6-bbef-777470f0003a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "cd peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fa00e8-d5aa-424c-b4f2-834efa227e7e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "!git checkout tags/v0.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b939dd3-c042-4b87-a7d2-d3f76479246d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6283eeff-3429-4a68-87bb-0dc8c56b49c4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1c744a-2f23-4234-8153-2f6c4a0fc86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd fairseq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9001656f-3f83-485c-8a98-5660b3043b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --no-cache-dir --editable ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2663e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip show fairseq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07416827-64d6-43d5-8ae4-46d6110f3362",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8db336",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd slam-llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60981a3c-67c1-4be3-87c9-b33865dcdbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip show slam-llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f69fd4-3a36-4383-b99d-bfad1071301d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install  -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea21a59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip uninstall slam-llm -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca2411c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/work/van-speech-nlp/jindaznb/slamenv/bin:/opt/miniconda/condabin:/opt/code-server-4.90.2-linux-amd64/lib/vscode/bin/remote-cli:/home/zhang.jinda1/espeak/bin:/usr/share/Modules/bin:/opt/miniconda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/home/zhang.jinda1/.local/bin:/home/zhang.jinda1/bin\n"
     ]
    }
   ],
   "source": [
    "!echo $PATH | grep espeak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a7897fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a26ffba6daa4ffb80cf65835224b513",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/214 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84ff691a60504dcc89ee05d57c5945ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/287 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbd7ccf854304efdab268d1e9a4297c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/486 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc3a9bb243e0458f984a1e77619ab23c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/23.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6597506432534b9bb9bfff06fe9764e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/309 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27f953f78dc84dc3b8c1c907de265064",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/2.07k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94c82859b9bc4a69968659133c439f6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.26G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at vitouphy/wav2vec2-xls-r-300m-timit-phoneme were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at vitouphy/wav2vec2-xls-r-300m-timit-phoneme and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC \n",
    "\n",
    "# load model and processor\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"vitouphy/wav2vec2-xls-r-300m-timit-phoneme\")\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"vitouphy/wav2vec2-xls-r-300m-timit-phoneme\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d38e1191",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at vitouphy/wav2vec2-xls-r-300m-timit-phoneme were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at vitouphy/wav2vec2-xls-r-300m-timit-phoneme and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pə']\n"
     ]
    }
   ],
   "source": [
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC \n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import soundfile as sf\n",
    "\n",
    "# load model and processor\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"vitouphy/wav2vec2-xls-r-300m-timit-phoneme\")\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"vitouphy/wav2vec2-xls-r-300m-timit-phoneme\")\n",
    "\n",
    "# Read and process the input\n",
    "audio_input, sample_rate = sf.read(\"/work/van-speech-nlp/data/torgo/F01/Session1/wav_arrayMic/0009.wav\")\n",
    "inputs = processor(audio_input, sampling_rate=16_000, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(inputs.input_values, attention_mask=inputs.attention_mask).logits\n",
    "\n",
    "# Decode id into string\n",
    "predicted_ids = torch.argmax(logits, axis=-1)      \n",
    "predicted_sentences = processor.batch_decode(predicted_ids)\n",
    "print(predicted_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b49ff4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Some weights of the model checkpoint at vitouphy/wav2vec2-xls-r-300m-timit-phoneme were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at vitouphy/wav2vec2-xls-r-300m-timit-phoneme and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Features Shape: torch.Size([1, 97, 1024])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
    "import soundfile as sf\n",
    "\n",
    "class Wav2PhonemeEncoder:\n",
    "\n",
    "    def __init__(self, processor, model):\n",
    "        self.processor = processor\n",
    "        self.model = model\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, model_config=None):\n",
    "        # Load the Wav2Vec2 processor and model\n",
    "        processor = Wav2Vec2Processor.from_pretrained(\"vitouphy/wav2vec2-xls-r-300m-timit-phoneme\")\n",
    "        model = Wav2Vec2ForCTC.from_pretrained(\"vitouphy/wav2vec2-xls-r-300m-timit-phoneme\")\n",
    "        return cls(processor, model)\n",
    "\n",
    "    def extract_features(self, audio_input):\n",
    "        # Process the audio input\n",
    "        inputs = self.processor(audio_input, sampling_rate=16_000, return_tensors=\"pt\", padding=True)\n",
    "        \n",
    "        # Make predictions with the model and obtain hidden states\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(input_values=inputs.input_values, attention_mask=inputs.attention_mask, output_hidden_states=True)\n",
    "            hidden_states = outputs.hidden_states\n",
    "        \n",
    "        # Extract the feature representation from the hidden states\n",
    "        # For instance, let's return the last hidden state\n",
    "        feature_representation = hidden_states[-1]  # Choose the appropriate layer if needed\n",
    "\n",
    "        return feature_representation\n",
    "\n",
    "def main():\n",
    "    # Load the encoder\n",
    "    encoder = Wav2PhonemeEncoder.load()\n",
    "\n",
    "    # Read an example audio file\n",
    "    audio_path = \"/work/van-speech-nlp/data/torgo/F01/Session1/wav_arrayMic/0009.wav\"  # Replace with the path to your audio file\n",
    "    audio_input, _ = sf.read(audio_path)\n",
    "\n",
    "    # Extract features (phoneme representations) from the audio\n",
    "    features = encoder.extract_features(audio_input)\n",
    "    print(\"Extracted Features Shape:\", features.shape)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94bb83bd",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'musicfm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 60\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtracted features:\u001b[39m\u001b[38;5;124m\"\u001b[39m, features)\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 60\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 44\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m wav_file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/work/van-speech-nlp/data/torgo/F01/Session1/wav_arrayMic/0009.wav\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Load the encoder\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m encoder \u001b[38;5;241m=\u001b[39m \u001b[43mMusicFMEncoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Load the WAV file\u001b[39;00m\n\u001b[1;32m     47\u001b[0m waveform, sample_rate \u001b[38;5;241m=\u001b[39m load_wav_file(wav_file_path)\n",
      "Cell \u001b[0;32mIn[6], line 15\u001b[0m, in \u001b[0;36mMusicFMEncoder.load\u001b[0;34m(cls, model_config)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\u001b[38;5;28mcls\u001b[39m, model_config):\n\u001b[0;32m---> 15\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmusicfm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmusicfm_25hz\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MusicFM25Hz\n\u001b[1;32m     16\u001b[0m     model \u001b[38;5;241m=\u001b[39m MusicFM25Hz(\n\u001b[1;32m     17\u001b[0m         stat_path\u001b[38;5;241m=\u001b[39mmodel_config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoder_stat_path\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     18\u001b[0m         model_path\u001b[38;5;241m=\u001b[39mmodel_config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoder_path\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     19\u001b[0m         w2v2_config_path\u001b[38;5;241m=\u001b[39mmodel_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoder_config_path\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacebook/wav2vec2-conformer-rope-large-960h-ft\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     20\u001b[0m     )\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(model_config, model)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'musicfm'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import json\n",
    "from pathlib import Path\n",
    "import torch.nn as nn\n",
    "\n",
    "class MusicFMEncoder(nn.Module):\n",
    "    def __init__(self, config, model):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.model = model\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, model_config):\n",
    "        from musicfm.model.musicfm_25hz import MusicFM25Hz\n",
    "        model = MusicFM25Hz(\n",
    "            stat_path=model_config['encoder_stat_path'],\n",
    "            model_path=model_config['encoder_path'],\n",
    "            w2v2_config_path=model_config.get('encoder_config_path', \"facebook/wav2vec2-conformer-rope-large-960h-ft\")\n",
    "        )\n",
    "        return cls(model_config, model)\n",
    "\n",
    "    def extract_features(self, source, padding_mask=None):\n",
    "        _, hidden_states = self.model.get_predictions(source)\n",
    "        out = hidden_states[self.config['encoder_layer_idx']]\n",
    "        return out\n",
    "\n",
    "def load_wav_file(file_path):\n",
    "    waveform, sample_rate = torchaudio.load(file_path)\n",
    "    return waveform, sample_rate\n",
    "\n",
    "def main():\n",
    "    # Example model configuration\n",
    "    model_config = {\n",
    "        'encoder_stat_path': 'path/to/encoder_stat',\n",
    "        'encoder_path': 'path/to/encoder_model',\n",
    "        'encoder_layer_idx': 12\n",
    "    }\n",
    "\n",
    "    # Path to your WAV file\n",
    "    wav_file_path = \"/work/van-speech-nlp/data/torgo/F01/Session1/wav_arrayMic/0009.wav\"\n",
    "\n",
    "    # Load the encoder\n",
    "    encoder = MusicFMEncoder.load(model_config)\n",
    "\n",
    "    # Load the WAV file\n",
    "    waveform, sample_rate = load_wav_file(wav_file_path)\n",
    "\n",
    "    # Ensure the waveform is in the correct format (batch size, num_channels, num_frames)\n",
    "    if waveform.dim() == 2:\n",
    "        waveform = waveform.unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    # Extract features\n",
    "    features = encoder.extract_features(waveform)\n",
    "\n",
    "    # Print the extracted features\n",
    "    print(\"Extracted features:\", features)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21dd4c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_239983/718986089.py:36: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  input_audio, sample_rate = librosa.load(\"/content/bla.wav\", sr=16000)\n",
      "/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/content/bla.wav'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLibsndfileError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/librosa/core/audio.py:176\u001b[0m, in \u001b[0;36mload\u001b[0;34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 176\u001b[0m     y, sr_native \u001b[38;5;241m=\u001b[39m \u001b[43m__soundfile_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mduration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m sf\u001b[38;5;241m.\u001b[39mSoundFileRuntimeError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;66;03m# If soundfile failed, try audioread instead\u001b[39;00m\n",
      "File \u001b[0;32m/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/librosa/core/audio.py:209\u001b[0m, in \u001b[0;36m__soundfile_load\u001b[0;34m(path, offset, duration, dtype)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;66;03m# Otherwise, create the soundfile object\u001b[39;00m\n\u001b[0;32m--> 209\u001b[0m     context \u001b[38;5;241m=\u001b[39m \u001b[43msf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSoundFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context \u001b[38;5;28;01mas\u001b[39;00m sf_desc:\n",
      "File \u001b[0;32m/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/soundfile.py:658\u001b[0m, in \u001b[0;36mSoundFile.__init__\u001b[0;34m(self, file, mode, samplerate, channels, subtype, endian, format, closefd)\u001b[0m\n\u001b[1;32m    656\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info \u001b[38;5;241m=\u001b[39m _create_info_struct(file, mode, samplerate, channels,\n\u001b[1;32m    657\u001b[0m                                  \u001b[38;5;28mformat\u001b[39m, subtype, endian)\n\u001b[0;32m--> 658\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode_int\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosefd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    659\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mset\u001b[39m(mode)\u001b[38;5;241m.\u001b[39missuperset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr+\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseekable():\n\u001b[1;32m    660\u001b[0m     \u001b[38;5;66;03m# Move write position to 0 (like in Python file objects)\u001b[39;00m\n",
      "File \u001b[0;32m/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/soundfile.py:1216\u001b[0m, in \u001b[0;36mSoundFile._open\u001b[0;34m(self, file, mode_int, closefd)\u001b[0m\n\u001b[1;32m   1215\u001b[0m     err \u001b[38;5;241m=\u001b[39m _snd\u001b[38;5;241m.\u001b[39msf_error(file_ptr)\n\u001b[0;32m-> 1216\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LibsndfileError(err, prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError opening \u001b[39m\u001b[38;5;132;01m{0!r}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname))\n\u001b[1;32m   1217\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode_int \u001b[38;5;241m==\u001b[39m _snd\u001b[38;5;241m.\u001b[39mSFM_WRITE:\n\u001b[1;32m   1218\u001b[0m     \u001b[38;5;66;03m# Due to a bug in libsndfile version <= 1.0.25, frames != 0\u001b[39;00m\n\u001b[1;32m   1219\u001b[0m     \u001b[38;5;66;03m# when opening a named pipe in SFM_WRITE mode.\u001b[39;00m\n\u001b[1;32m   1220\u001b[0m     \u001b[38;5;66;03m# See http://github.com/erikd/libsndfile/issues/77.\u001b[39;00m\n",
      "\u001b[0;31mLibsndfileError\u001b[0m: Error opening '/content/bla.wav': System error.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 36\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlibrosa\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Load test audio\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m input_audio, sample_rate \u001b[38;5;241m=\u001b[39m \u001b[43mlibrosa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/content/bla.wav\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Load model and feature extractor\u001b[39;00m\n\u001b[1;32m     39\u001b[0m model_config \u001b[38;5;241m=\u001b[39m {}  \u001b[38;5;66;03m# Provide necessary config if needed\u001b[39;00m\n",
      "File \u001b[0;32m/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/librosa/core/audio.py:184\u001b[0m, in \u001b[0;36mload\u001b[0;34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, (\u001b[38;5;28mstr\u001b[39m, pathlib\u001b[38;5;241m.\u001b[39mPurePath)):\n\u001b[1;32m    181\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    182\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPySoundFile failed. Trying audioread instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[1;32m    183\u001b[0m     )\n\u001b[0;32m--> 184\u001b[0m     y, sr_native \u001b[38;5;241m=\u001b[39m \u001b[43m__audioread_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mduration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/decorator.py:232\u001b[0m, in \u001b[0;36mdecorate.<locals>.fun\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwsyntax:\n\u001b[1;32m    231\u001b[0m     args, kw \u001b[38;5;241m=\u001b[39m fix(args, kw, sig)\n\u001b[0;32m--> 232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcaller\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mextras\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/librosa/util/decorators.py:59\u001b[0m, in \u001b[0;36mdeprecated.<locals>.__wrapper\u001b[0;34m(func, *args, **kwargs)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Warn the user, and then proceed.\"\"\"\u001b[39;00m\n\u001b[1;32m     51\u001b[0m warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{:s}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{:s}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mDeprecated as of librosa version \u001b[39m\u001b[38;5;132;01m{:s}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mIt will be removed in librosa version \u001b[39m\u001b[38;5;132;01m{:s}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     57\u001b[0m     stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,  \u001b[38;5;66;03m# Would be 2, but the decorator adds a level\u001b[39;00m\n\u001b[1;32m     58\u001b[0m )\n\u001b[0;32m---> 59\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/librosa/core/audio.py:240\u001b[0m, in \u001b[0;36m__audioread_load\u001b[0;34m(path, offset, duration, dtype)\u001b[0m\n\u001b[1;32m    237\u001b[0m     reader \u001b[38;5;241m=\u001b[39m path\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;66;03m# If the input was not an audioread object, try to open it\u001b[39;00m\n\u001b[0;32m--> 240\u001b[0m     reader \u001b[38;5;241m=\u001b[39m \u001b[43maudioread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maudio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m reader \u001b[38;5;28;01mas\u001b[39;00m input_file:\n\u001b[1;32m    243\u001b[0m     sr_native \u001b[38;5;241m=\u001b[39m input_file\u001b[38;5;241m.\u001b[39msamplerate\n",
      "File \u001b[0;32m/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/audioread/__init__.py:127\u001b[0m, in \u001b[0;36maudio_open\u001b[0;34m(path, backends)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m BackendClass \u001b[38;5;129;01min\u001b[39;00m backends:\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 127\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mBackendClass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m DecodeError:\n\u001b[1;32m    129\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/audioread/rawread.py:59\u001b[0m, in \u001b[0;36mRawAudioFile.__init__\u001b[0;34m(self, filename)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, filename):\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fh \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_file \u001b[38;5;241m=\u001b[39m aifc\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fh)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/bla.wav'"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import torch\n",
    "from transformers import Wav2Vec2FeatureExtractor, Wav2Vec2Model\n",
    "\n",
    "input_audio, sample_rate = librosa.load(\"/work/van-speech-nlp/data/torgo/F01/Session1/wav_arrayMic/0006.wav\",  sr=16000)\n",
    "\n",
    "model_name = \"vitouphy/wav2vec2-xls-r-300m-timit-phoneme\"\n",
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(model_name)\n",
    "model = Wav2Vec2Model.from_pretrained(model_name)\n",
    "\n",
    "i= feature_extractor(input_audio, return_tensors=\"pt\", sampling_rate=sample_rate)\n",
    "with torch.no_grad():\n",
    "  o= model(i.input_values)\n",
    "print(o.keys())\n",
    "print(o.last_hidden_state.shape)\n",
    "print(o.extract_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "119de635",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at vitouphy/wav2vec2-xls-r-300m-timit-phoneme and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/transformers/feature_extraction_utils.py:93\u001b[0m, in \u001b[0;36mBatchFeature.__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "\u001b[0;31mKeyError\u001b[0m: 'shape'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 43\u001b[0m\n\u001b[1;32m     40\u001b[0m encoder \u001b[38;5;241m=\u001b[39m Wav2PhonemeEncoder\u001b[38;5;241m.\u001b[39mload(model_config)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Extract features\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[43mencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_audio\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Print outputs\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeature shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, features\u001b[38;5;241m.\u001b[39mshape)\n",
      "Cell \u001b[0;32mIn[7], line 22\u001b[0m, in \u001b[0;36mWav2PhonemeEncoder.extract_features\u001b[0;34m(self, audio_input)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_features\u001b[39m(\u001b[38;5;28mself\u001b[39m, audio_input):\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;66;03m# Process audio_input\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_extractor(audio_input, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, sampling_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16000\u001b[39m)\n\u001b[0;32m---> 22\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minputs\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     24\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n",
      "File \u001b[0;32m/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/transformers/feature_extraction_utils.py:95\u001b[0m, in \u001b[0;36mBatchFeature.__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[item]\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[0;32m---> 95\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import Wav2Vec2FeatureExtractor, Wav2Vec2Model\n",
    "\n",
    "class Wav2PhonemeEncoder(nn.Module):\n",
    "    def __init__(self, config, model, feature_extractor):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.model = model\n",
    "        self.feature_extractor = feature_extractor\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, model_config):\n",
    "        # Load feature extractor and model\n",
    "        feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\"vitouphy/wav2vec2-xls-r-300m-timit-phoneme\")\n",
    "        model = Wav2Vec2Model.from_pretrained(\"vitouphy/wav2vec2-xls-r-300m-timit-phoneme\")\n",
    "        return cls(model_config, model, feature_extractor)\n",
    "\n",
    "    def extract_features(self, audio_input):\n",
    "        # Process audio_input\n",
    "        inputs = self.feature_extractor(audio_input, return_tensors=\"pt\", sampling_rate=16000)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "        \n",
    "        # Extract features\n",
    "        features = outputs.last_hidden_state\n",
    "        \n",
    "        return features\n",
    "\n",
    "# Test the updated encoder class\n",
    "if __name__ == \"__main__\":\n",
    "    import librosa\n",
    "\n",
    "    # Load test audio\n",
    "    input_audio, sample_rate = librosa.load(\"/work/van-speech-nlp/data/torgo/F01/Session1/wav_arrayMic/0006.wav\", sr=16000)\n",
    "    \n",
    "    # Load model and feature extractor\n",
    "    model_config = {}  # Provide necessary config if needed\n",
    "    encoder = Wav2PhonemeEncoder.load(model_config)\n",
    "\n",
    "    # Extract features\n",
    "    features = encoder.extract_features(input_audio)\n",
    "\n",
    "    # Print outputs\n",
    "    print(\"Feature shape:\", features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775ccf46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
