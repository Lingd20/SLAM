{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898410a4-7512-43c0-bcd6-ee6daa8652d9",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "cd transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ca2b9d-67b9-4841-904e-2497b2da4cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f412b60e-76dc-40d7-af96-61e15c266a6f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "!git checkout tags/v4.35.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54455652",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip show transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdba35ec-495f-45ca-afb3-b4cc1ef028a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2052fec-e6c5-4bd7-9fb7-9e6ff91dd21e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29cde01-5f51-45d6-bbef-777470f0003a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "cd peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fa00e8-d5aa-424c-b4f2-834efa227e7e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "!git checkout tags/v0.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b939dd3-c042-4b87-a7d2-d3f76479246d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6283eeff-3429-4a68-87bb-0dc8c56b49c4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1c744a-2f23-4234-8153-2f6c4a0fc86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd fairseq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9001656f-3f83-485c-8a98-5660b3043b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --no-cache-dir --editable ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2663e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip show fairseq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07416827-64d6-43d5-8ae4-46d6110f3362",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8db336",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd slam-llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60981a3c-67c1-4be3-87c9-b33865dcdbbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: slam-llm\n",
      "Version: 0.0.1\n",
      "Summary: SLAM-LLM is a deep learning toolkit that allows researchers and developers to train custom multimodal large language model (MLLM), focusing on Speech, Language, Audio, Music processing. We provide detailed recipes for training and high-performance checkpoints for inference.\n",
      "Home-page: \n",
      "Author: \n",
      "Author-email: \n",
      "License: \n",
      "Location: /work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages\n",
      "Editable project location: /work/van-speech-nlp/jindaznb/jslpnb/mllm_expriments/slam-llm\n",
      "Requires: accelerate, appdirs, bitsandbytes, black, black, datasets, fire, hydra-core, loralib, optimum, peft, py7zr, scipy, sentencepiece, torch, transformers, wandb\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show slam-llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f69fd4-3a36-4383-b99d-bfad1071301d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install  -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea21a59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip uninstall slam-llm -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0245dc7c-58e8-4ed9-b488-d708e8a9f562",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def factorial(n):\n",
    "    if n == 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return n * factorial(n-1)\n",
    "\n",
    "def main():\n",
    "    number = 5\n",
    "    result = factorial(number)\n",
    "    print(f\"The factorial of {number} is {result}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import pdb; pdb.set_trace()  # Set a breakpoint here\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecff0953-8160-475c-afad-00b7d16d3b50",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def divide(a, b):\n",
    "    return a / b\n",
    "\n",
    "def main():\n",
    "    x = 10\n",
    "    y = 0\n",
    "    result = divide(x, y)\n",
    "    print(f\"The result is: {result}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9973b9-b4df-421b-8340-536ca3a6669e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d221e7f-ce58-4939-86cc-eb5c4d61d72b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%pdb on\n",
    "\n",
    "def my_function(x):\n",
    "    result = x / 0  # 这将引发一个除零错误\n",
    "    return result\n",
    "\n",
    "my_function(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d56917-f4c6-4369-a3b9-6f32d671b518",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import ipdb\n",
    "\n",
    "def my_function(x):\n",
    "    ipdb.set_trace()  # 设置断点\n",
    "    result = x * x\n",
    "    return result\n",
    "\n",
    "my_function(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdbfc44-92e2-4c06-8303-a76bb80b0248",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "!pip install ipdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb913bb4-ed98-4d15-8c7e-4a3bc587062b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pocketsphinx import AudioFile, get_model_path\n",
    "\n",
    "# 设置模型路径\n",
    "model_path = get_model_path()\n",
    "\n",
    "# 定义音频文件路径\n",
    "audio_file = 'path_to_your_audio_file.wav'\n",
    "\n",
    "# 配置 pocketsphinx\n",
    "config = {\n",
    "    'verbose': False,\n",
    "    'audio_file': audio_file,\n",
    "    'hmm': os.path.join(model_path, 'en-us'),\n",
    "    'dict': os.path.join(model_path, 'cmudict-en-us.dict'),\n",
    "    'lm': False,\n",
    "    'allphone': os.path.join(model_path, 'en-us-phone.lm.bin'),\n",
    "}\n",
    "\n",
    "# 创建 AudioFile 对象\n",
    "audio = AudioFile(**config)\n",
    "\n",
    "# 提取音素\n",
    "for phrase in audio:\n",
    "    print(phrase.segments(detailed=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd0325a-9e06-4811-8f9f-bc7cca71bcb8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "!pip uninstall pocketsphinx -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d92e6e-8c77-472c-a4c0-927aebeceb4e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "import torch\n",
    "import torchaudio\n",
    "\n",
    "# 加载预训练模型和处理器\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "\n",
    "# 加载音频文件\n",
    "speech_array, sampling_rate = torchaudio.load(\"/work/van-speech-nlp/data/torgo//F01/Session1/wav_arrayMic/0006.wav\")\n",
    "\n",
    "# 预处理音频\n",
    "input_values = processor(speech_array, sampling_rate=sampling_rate, return_tensors=\"pt\").input_values\n",
    "\n",
    "# 执行模型推理\n",
    "with torch.no_grad():\n",
    "    logits = model(input_values).logits\n",
    "\n",
    "# 获取预测的音素索引\n",
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "\n",
    "# 解码音素\n",
    "transcription = processor.batch_decode(predicted_ids)\n",
    "print(transcription)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3195fb78-8815-4ede-a29e-712fb9de946e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install allosaurus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30547e9-cbf2-4802-b67d-ac500c068c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from allosaurus.app import read_recognizer\n",
    "from allosaurus.model import resolve_model_name\n",
    "\n",
    "# load model\n",
    "model_path = resolve_model_name('latest')\n",
    "recognizer = read_recognizer(model_path)\n",
    "\n",
    "# load your audio file\n",
    "audio_path = \"/work/van-speech-nlp/data/torgo//F01/Session1/wav_arrayMic/0006.wav\"\n",
    "\n",
    "# recognize phonemes\n",
    "result = recognizer.recognize(audio_path)\n",
    "\n",
    "# print phonemes\n",
    "for sentence in result:\n",
    "    print(sentence.get_tokens())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b9cdd7-2393-48ce-99ab-52a9b10950f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_path = \"/work/van-speech-nlp/data/torgo//F01/Session1/wav_arrayMic/0006.wav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd556d6-499b-4865-b078-2d027a89cb95",
   "metadata": {},
   "outputs": [],
   "source": [
    " from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
    " from datasets import load_dataset\n",
    " import torch\n",
    " \n",
    " # load model and processor\n",
    " processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-xlsr-53-espeak-cv-ft\")\n",
    " model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-xlsr-53-espeak-cv-ft\")\n",
    "     \n",
    " # load dummy dataset and read soundfiles\n",
    " ds = load_dataset(\"patrickvonplaten/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n",
    " \n",
    " # tokenize\n",
    " input_values = processor(ds[0][\"audio\"][\"array\"], return_tensors=\"pt\").input_values\n",
    " \n",
    " # retrieve logits\n",
    " with torch.no_grad():\n",
    "   logits = model(input_values).logits\n",
    " \n",
    " # take argmax and decode\n",
    " predicted_ids = torch.argmax(logits, dim=-1)\n",
    " transcription = processor.batch_decode(predicted_ids)\n",
    " # => should give ['m ɪ s t ɚ k w ɪ l t ɚ ɪ z ð ɪ ɐ p ɑː s əl l ʌ v ð ə m ɪ d əl k l æ s ɪ z æ n d w iː aʊ ɡ l æ d t ə w ɛ l k ə m h ɪ z ɡ ɑː s p ə']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d538d7-8cad-486a-9128-ee6408c3f9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install python-espeak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d2ebf1-88dc-47ef-9455-d295a0641982",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install soundfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64736fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a7b01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall whisper -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0c7f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai-whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e132330",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
